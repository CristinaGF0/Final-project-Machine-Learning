{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1P_0Eg7sKFr",
        "outputId": "0ad50fa3-7552-4fde-fdb5-3dc6289e7974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('Attrition', axis=1)\n",
        "y = data['Attrition']\n",
        "\n",
        "# Define categorical and numerical column\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create transformers for numerical and categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply the transformation to the data\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "TrFL-Iu0-5BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RANDOM FOREST"
      ],
      "metadata": {
        "id": "ZbPKhhK6PdvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Define the parameters for the search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],   # Number of features to consider at each split\n",
        "    'max_depth': [None, 10, 20, 30],   # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],   # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4] } # Minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found:\")\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4a7k_J-5MA",
        "outputId": "064f11f1-5a32-4744-e310-3a36c13d212e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "540 fits failed out of a total of 1620.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "540 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.85714749 0.86054093 0.85714028\n",
            " 0.85799495 0.86054814 0.85798774 0.85544537 0.85799495 0.85884241\n",
            " 0.85459791 0.85628922 0.85544176 0.85969708 0.85543815 0.85630004\n",
            " 0.85799135 0.8545943  0.85629643 0.85628922 0.8503462  0.8545943\n",
            " 0.85289578 0.85374324 0.85374324 0.8588388  0.85289217 0.85374684\n",
            " 0.85458709 0.85714389 0.85204472 0.8545943  0.85714028 0.85204472\n",
            " 0.84949153 0.85119365 0.85289217 0.85714028 0.85374324 0.85289217\n",
            " 0.85373963 0.85375045 0.85374684 0.85289217 0.85289217 0.85119365\n",
            " 0.85119005 0.84779661 0.8503462  0.85289217 0.84864767 0.85119005\n",
            " 0.85204111 0.8503462  0.84949513        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.8529066  0.85714749 0.85799495 0.85884241 0.8571511  0.85544537\n",
            " 0.85884241 0.85884962 0.85714389 0.85544176 0.85884962 0.85714028\n",
            " 0.85629282 0.85798774 0.85544176 0.85204472 0.85544897 0.85714749\n",
            " 0.85119365 0.85544176 0.85459791 0.85630004 0.85629282 0.85374684\n",
            " 0.85629282 0.85459791 0.85289578 0.8545943  0.85374684 0.85289939\n",
            " 0.85289939 0.85289578 0.85119365 0.85544537 0.85119726 0.85120087\n",
            " 0.8520375  0.85544897 0.85119726 0.85204472 0.8545943  0.85374684\n",
            " 0.85204472 0.85289578 0.84949153 0.85120087 0.84779661 0.85119365\n",
            " 0.85119005 0.84694555 0.84949874 0.84694194 0.84949153 0.84865128\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.85543815 0.85374324 0.85714028\n",
            " 0.85799856 0.85713668 0.85714389 0.86139921 0.85629643 0.8588352\n",
            " 0.86054814 0.85544537 0.85884962 0.8545907  0.85969708 0.85969347\n",
            " 0.85714749 0.85373963 0.85119005 0.85629282 0.85629643 0.85544897\n",
            " 0.85544176 0.8545907  0.85884602 0.8545943  0.85119726 0.85375045\n",
            " 0.85544537 0.85544537 0.85544897 0.85629282 0.85459791 0.85459791\n",
            " 0.85544897 0.85374324 0.85459791 0.85374684 0.85119365 0.84949513\n",
            " 0.85034259 0.84864767 0.85289939 0.85289939 0.847793   0.85375045\n",
            " 0.84779661 0.85204111 0.847793   0.84609088 0.84864407 0.8503462\n",
            " 0.85119365 0.85034259 0.85119726        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.85629282 0.85884241 0.85714749 0.85714389 0.85970069 0.85799135\n",
            " 0.86054093 0.85884241 0.85544537 0.85884602 0.85628922 0.8545907\n",
            " 0.85714749 0.85460151 0.85630004 0.85969708 0.8520375  0.85714749\n",
            " 0.8545943  0.85374684 0.85289217 0.8545907  0.85374324 0.85289578\n",
            " 0.85459791 0.85714749 0.85544537 0.85544537 0.85544897 0.85289217\n",
            " 0.85545618 0.85374324 0.85204111 0.85375045 0.8503498  0.85119365\n",
            " 0.85629282 0.85544176 0.85544537 0.85629282 0.85120087 0.85289217\n",
            " 0.85544176 0.85119726 0.85288857 0.847793   0.85288857 0.84864767\n",
            " 0.85119005 0.85119365 0.84949874 0.84694915 0.84949513 0.84864767]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros encontrados:\n",
            "{'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Get the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8NhSoeX-5PY",
        "outputId": "396099cb-1632-48f0-d893-fd6729716655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8707482993197279\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          No       0.88      0.99      0.93       255\n",
            "         Yes       0.57      0.10      0.17        39\n",
            "\n",
            "    accuracy                           0.87       294\n",
            "   macro avg       0.72      0.55      0.55       294\n",
            "weighted avg       0.84      0.87      0.83       294\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "L3LGKd2-POSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "log_reg = LogisticRegression(max_iter=1000)  # max_iter es importante para asegurar la convergencia\n",
        "\n",
        "# Define the parameters for the search\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None],  # Tipo de regularización\n",
        "    'C': [0.1, 1, 10, 100],  # Inverso de la regularización\n",
        "    'solver': ['liblinear', 'saga'],  # Algoritmos para resolver la regresión logística\n",
        "    'class_weight': [None, 'balanced']  # Pesos para las clases desbalanceadas\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "\n",
        "grid_search_log_reg = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search_log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found:\")\n",
        "print(grid_search_log_reg.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u92cB2UbCGpU",
        "outputId": "b21e8a6d-d4d6-4aaa-f948-adfb874ce532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
            "Mejores parámetros encontrados:\n",
            "{'C': 0.1, 'class_weight': None, 'penalty': 'l2', 'solver': 'liblinear'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "120 fits failed out of a total of 320.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "40 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1169, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 66, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "40 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1179, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "40 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1228, in fit\n",
            "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\", line 1229, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\", line 1060, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.85545618 0.85545979 0.87416516 0.87161197        nan        nan\n",
            "        nan 0.86310855 0.76358817 0.76358817 0.75936531 0.76021637\n",
            "        nan        nan        nan 0.76020555 0.86734223 0.87074288\n",
            " 0.86225027 0.86225027        nan        nan        nan 0.86310855\n",
            " 0.76361702 0.76361702 0.76021637 0.76021637        nan        nan\n",
            "        nan 0.76020555 0.86310855 0.86225748 0.86140642 0.86140642\n",
            "        nan        nan        nan 0.86310855 0.76020916 0.76106022\n",
            " 0.76020916 0.76020916        nan        nan        nan 0.76020555\n",
            " 0.86310855 0.86310855 0.86310855 0.86310855        nan        nan\n",
            "        nan 0.86310855 0.76020555 0.76020555 0.76020555 0.76020555\n",
            "        nan        nan        nan 0.76020555]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Get the best model\n",
        "best_log_reg = grid_search_log_reg.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_log_reg = best_log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_log_reg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvkzgTwNCGss",
        "outputId": "f0c3e632-b095-44a9-fd49-ecb346c44f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9013605442176871\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          No       0.91      0.98      0.95       255\n",
            "         Yes       0.78      0.36      0.49        39\n",
            "\n",
            "    accuracy                           0.90       294\n",
            "   macro avg       0.84      0.67      0.72       294\n",
            "weighted avg       0.89      0.90      0.89       294\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CATBOOST"
      ],
      "metadata": {
        "id": "6kMdAeMCPCKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize CatBoost\n",
        "catboost_model = CatBoostClassifier(learning_rate=0.1, depth=6, iterations=500, silent=True)\n",
        "\n",
        "# Define the parameters for the search\n",
        "param_grid_catboost = {\n",
        "    'iterations': [100, 200, 300],  # Number of iterations (trees)\n",
        "    'depth': [4, 6, 8],  # Depth of the trees\n",
        "    'learning_rate': [0.01, 0.1, 0.2],   # Learning rate\n",
        "    'l2_leaf_reg': [1, 3, 5], # L2 regularization\n",
        "    'subsample': [0.7, 0.8, 0.9], # Fraction of samples used to fit each tree\n",
        "    'auto_class_weights': [None, 'Balanced']  # Handling of class imbalance\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "grid_search_catboost = GridSearchCV(estimator=catboost_model, param_grid=param_grid_catboost, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search_catboost.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found:\")\n",
        "print(grid_search_catboost.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbTXrumjEL7l",
        "outputId": "949d2e39-d1d4-4856-acdd-eeb5809ce0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "1215 fits failed out of a total of 2430.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "1215 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/catboost/core.py\", line 5220, in fit\n",
            "    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/catboost/core.py\", line 2385, in _fit\n",
            "    train_params = self._prepare_train_params(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/catboost/core.py\", line 2311, in _prepare_train_params\n",
            "    _check_train_params(params)\n",
            "  File \"_catboost.pyx\", line 6393, in _catboost._check_train_params\n",
            "  File \"_catboost.pyx\", line 6415, in _catboost._check_train_params\n",
            "_catboost.CatBoostError: /src/catboost/catboost/private/libs/options/json_helper.h:41: Can't parse parameter \"auto_class_weights\" with value: null\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.77888568 0.77719437 0.78058781\n",
            " 0.8426758  0.84013343 0.82992427 0.85372881 0.847793   0.85118644\n",
            " 0.78144248 0.7763397  0.77634331 0.83842048 0.83843491 0.8358709\n",
            " 0.85202308 0.85373603 0.85033177 0.77888929 0.77717995 0.77463758\n",
            " 0.83588893 0.82991345 0.82737108 0.85033898 0.84269383 0.84776776\n",
            " 0.79843851 0.79249189 0.79164803 0.84438875 0.84438875 0.85205193\n",
            " 0.86138478 0.85712586 0.84949513 0.78824739 0.79419401 0.78824378\n",
            " 0.85712586 0.8562784  0.85628922 0.84692752 0.8588352  0.85714028\n",
            " 0.78739632 0.79164082 0.78994951 0.8562784  0.85116841 0.85287775\n",
            " 0.85118644 0.84778219 0.85287414 0.80099531 0.80781825 0.79761269\n",
            " 0.84779661 0.85629643 0.85204111 0.86563649 0.85542373 0.85119365\n",
            " 0.80102056 0.80525784 0.79590335 0.86733141 0.85712586 0.85628201\n",
            " 0.85712946 0.85627479 0.85628922 0.7950559  0.79844933 0.79675802\n",
            " 0.85798774 0.85288496 0.85373242 0.85797692 0.85458709 0.85117562\n",
            " 0.80438154 0.79758024 0.80269383 0.85459791 0.85627479 0.85800938\n",
            " 0.84780022 0.85799135 0.84864407 0.79589975 0.79843851 0.80439596\n",
            " 0.85628561 0.85627119 0.84693473 0.84864407 0.85288857 0.85119005\n",
            " 0.79588893 0.7941832  0.79419041 0.85458348 0.85120087 0.84949153\n",
            " 0.85716192 0.85289578 0.85714389 0.82819329 0.8197115  0.81969708\n",
            " 0.85460512 0.85969347 0.85715831 0.85119005 0.86309773 0.85458348\n",
            " 0.81716552 0.81631085 0.81460512 0.86053732 0.8605229  0.85288136\n",
            " 0.85288136 0.85457627 0.85630004 0.80950956 0.81035341 0.81289939\n",
            " 0.85543815 0.85884602 0.85034259 0.86140642 0.8545943  0.85630004\n",
            " 0.83840966 0.8384313  0.82821854 0.85629643 0.85884602 0.84865489\n",
            " 0.85119005 0.86394879 0.85288496 0.82396322 0.83076812 0.82821854\n",
            " 0.86138839 0.85968265 0.85373963 0.85373603 0.85883159 0.85629643\n",
            " 0.82225027 0.81716192 0.82055536 0.85628922 0.85374684 0.85034259\n",
            " 0.85885323 0.85969708 0.85968626 0.84010819 0.84435629 0.84265777\n",
            " 0.86734944 0.85544176 0.85628922 0.85631446 0.85289578 0.85630004\n",
            " 0.8265092  0.8213956  0.82223945 0.85884241 0.8588388  0.86139199\n",
            " 0.85886044 0.85034259 0.86223585 0.81544897 0.81373603 0.8145907\n",
            " 0.85884241 0.86394519 0.85714028 0.86054814 0.8520339  0.85204111\n",
            " 0.85117562 0.86138478 0.85713307 0.86394879 0.85374684 0.86140281\n",
            " 0.85970429 0.85714749 0.85545258 0.8426758  0.84352326 0.84946268\n",
            " 0.85969347 0.86138478 0.85543815 0.85970429 0.85119726 0.85798053\n",
            " 0.82991345 0.8333141  0.83840966 0.85543815 0.86223945 0.85628922\n",
            " 0.86138839 0.84949874 0.8545907  0.85629643 0.86223585 0.85798413\n",
            " 0.86309773 0.85714749 0.86054814 0.86055536 0.85714749 0.85545258\n",
            " 0.85458348 0.85627479 0.85032456 0.86054093 0.86477822 0.85628561\n",
            " 0.85970069 0.85119726 0.86138118 0.84606924 0.84946268 0.84862604\n",
            " 0.85543815 0.86054454 0.85969347 0.86223945 0.85714028 0.85714749]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros encontrados:\n",
            "{'auto_class_weights': 'Balanced', 'depth': 8, 'iterations': 100, 'l2_leaf_reg': 1, 'learning_rate': 0.1, 'subsample': 0.7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Get the best model\n",
        "best_catboost = grid_search_catboost.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_catboost = best_catboost.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_catboost))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_catboost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3MErt6eN6OB",
        "outputId": "427ba5cc-6b42-4b50-8ea9-02877d34cc8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8877551020408163\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          No       0.89      0.99      0.94       255\n",
            "         Yes       0.75      0.23      0.35        39\n",
            "\n",
            "    accuracy                           0.89       294\n",
            "   macro avg       0.82      0.61      0.65       294\n",
            "weighted avg       0.87      0.89      0.86       294\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### So far, the best result is obtained from the hyperparameterized logistic regression, but we are encountering the problem of class imbalance. The model currently predicts employees who do not experience attrition better than those who do."
      ],
      "metadata": {
        "id": "RrDtA3T68Oke"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TaXdtk52N6WY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}